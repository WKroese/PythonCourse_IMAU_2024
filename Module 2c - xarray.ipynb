{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e3d035",
   "metadata": {},
   "source": [
    "# 2c) xarray\n",
    "\n",
    "This tutorial is adapted in large part from the [xarray FAQ](http://xarray.pydata.org/en/stable/faq) and the [Earth and Environmental Data Science course's xarray lecture](https://earth-env-data-science.github.io/lectures/xarray/xarray.html). Also, part of this tutorial is based on the Python introduction from last years of André Jüling, created under the BSD 3-Clause License (https://github.com/AJueling/python_climate_physics), and Janneke Krabbendam.\n",
    "\n",
    "We have already introduced NetCDF files and the NetCDF4 package. The latter is only used for reading and creating NetCDF files. There is also a \"pandas-like\" package for NetCDF files: __Xarray__ was designed to make reading netCDF files in python as easy, powerful, and flexible as possible. (See [xarray netCDF docs](http://xarray.pydata.org/en/latest/io.html#netcdf) for more details.) xarray builds on NetCDF4.\n",
    "\n",
    "The first part of the tutorial will focus on working with Pandas, the second part on NetCDF4 and xarray. First, let's import the package\n",
    "\n",
    "`import xarray as xr`\n",
    "\n",
    "You can check wether pandas is alreay installed on your pc with the command\n",
    "\n",
    "`conda list`\n",
    "\n",
    "This gives an overview of all the packages that are installed. If you don't have pandas yet, install it with\n",
    "\n",
    "`conda install xarray` or `pip install xarray` \n",
    "\n",
    "You can do this directly in the Notebook or in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26499f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90663a4d",
   "metadata": {},
   "source": [
    "### xarray Terminology\n",
    "\n",
    "First we need to establish the basic concepts. Imagine ocean model output with temperature and salinity stored at (x,y,z,t).\n",
    "\n",
    "Similar to pandas, there are two data structures available in xarray:\n",
    "- `DataArray`: is single variable field, e.g. temperature that can be high dimensional at each point for several time steps.\n",
    "- `Dataset`: holds several DataArrays, e.g. both the temperature and salinity fields. These DataArrays may be aligned, i.e. be located at the same points.\n",
    "\n",
    "A `DataArray` has four essential attributes:\n",
    "* `values`: a `numpy.ndarray` holding the array’s values, e.g. temperature or salinity values\n",
    "* `dims`: dimension names for each axis (e.g., `('x', 'y', 'z')`, or `('lat', 'lon', 'time')`)\n",
    "* `coords`: a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings), e.g. the locations in a spatial direction: 1 m, 2 m, 3 m, ... or in time: day 1, day 2, day 3, ...\n",
    "* `attrs`: an `OrderedDict` to hold arbitrary metadata (attributes)\n",
    "\n",
    "## DataArray\n",
    "We will first investigate a DataArray by creating one filled with random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80419751",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(np.random.randn(2, 3), dims=(\"x\", \"y\"), coords={\"x\": [10, 20]})\n",
    "da  # this produces a fancy html representation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427f5d1",
   "metadata": {},
   "source": [
    "We have generated a 2D array, assigned the names x and y to the two dimensions respectively and associated two coordinate labels ‘10’ and ‘20’ with the two locations along the x dimension. Notice not every dimension needs to have a coordinate.\n",
    "\n",
    "These are the key properties of the DataArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(da.values)\n",
    "print(da.dims)\n",
    "print(da.coords)\n",
    "print(da.attrs)  # empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be939e61",
   "metadata": {},
   "source": [
    "Xarray has built-in plotting, like pandas. We can modify these plots with extra matplotlib code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot()  # notice the automatic labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3094080",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "da.plot(ax=ax[0])\n",
    "ax[0].set_title('some random data')\n",
    "abs(da).plot(ax=ax[1])\n",
    "ax[1].set_title('absolute values')\n",
    "ax[1].set_xlabel('some other x label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6aee8f",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Indexing options are similar to what we have seen in NumPy and Pandas, but there are more options. Such as `sel` (=select) and `isel` (=integer select), in the first case you give a coordinate and in the second case an integer. What makes these options so useful, is that the values don't need to be exact, but you can tell Python to return the values closest.\n",
    "\n",
    "With this label-based indexing, you don't have to know how an array is organized. You just need to know  the dimension name and the label we wish to index i.e. `data.sel(x=10)` works regardless of whether `x` is the first or second dimension of the array and regardless of whether 10 is the first or second element of `x`. We have already told xarray that `x` is the first dimension when we created `data`: xarray keeps track of this so we don’t have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4dcf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional and by integer label, like numpy\n",
    "print(da[0, :], '\\n\\n')\n",
    "\n",
    "# loc or \"location\": positional and coordinate label, like pandas\n",
    "print(da.loc[10], '\\n\\n')\n",
    "\n",
    "# isel or \"integer select\":  by dimension name and integer label\n",
    "print(da.isel(x=0), '\\n\\n')\n",
    "\n",
    "# sel or \"select\": by dimension name and coordinate label\n",
    "print(da.sel(x=10), '\\n\\n')\n",
    "\n",
    "# the values do not need to be exact\n",
    "# this is extremely handy for selecting the closest location, for example\n",
    "print(da.sel(x=12, method='nearest'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2809b47",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now we move on to Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba4f77",
   "metadata": {},
   "source": [
    "A Dataset holds many DataArrays which potentially can share coordinates. In analogy to pandas:\n",
    "\n",
    "    pandas.Series : xarray.DataArray\n",
    "    pandas.Dataframe  : xarray.Dataset\n",
    "    \n",
    "Constructing Datasets manually is a bit more involved in terms of syntax. The `Dataset` constructor takes three arguments:\n",
    "\n",
    "* `data_vars` should be a dictionary with each key as the name of the variable and each value as one of:\n",
    "  * A `DataArray` or Variable\n",
    "  * A tuple of the form `(dims, data[, attrs])`, which is converted into arguments for Variable\n",
    "  * A pandas object, which is converted into a `DataArray`\n",
    "  * A 1D array or list, which is interpreted as values for a one dimensional coordinate variable along the same dimension as it’s name\n",
    "* `coords` should be a dictionary of the same form as data_vars.\n",
    "* `attrs` should be a dictionary.\n",
    "\n",
    "We can create a Dataset in [several ways](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.html), here is one using a distionary with names and DataArrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simplified way to construct a dataset is to provide a dictionry with the names and DataArrays:\n",
    "xr.Dataset({'data':da, 'data_squared':da**2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dc050",
   "metadata": {},
   "source": [
    "### Open netCDF files\n",
    "Opening netCDF files is a simple command with xarray: `open_dataset`\n",
    "\n",
    "Let us first download some the NASA [GISSTemp](https://data.giss.nasa.gov/gistemp/) global temperature anomaly dataset. This file was a bit big to upload to GitHub, so go to the [website](https://data.giss.nasa.gov/pub/gistemp/) and download (and unzip) `gistemp1200_ERSSTv5.nc.gz` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data/gistemp1200_GHCNv4_ERSSTv5.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73288219",
   "metadata": {},
   "source": [
    "Inspect the contents of this file by typing the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5293432",
   "metadata": {},
   "source": [
    "Like pandas, xarray has built-in plot functions, which makes it easy to investigate the data. With the indexing options, we can easily plot the temperature anomaly on the first of june:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tempanomaly.sel(time='2020-06-01', method='nearest').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a2c16",
   "metadata": {},
   "source": [
    "## Creating netCDF files and saving them\n",
    "Above we created the DataArray `da` which we can save with a single command as a netcdf file (this also works for DataSets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.to_netcdf('test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5785228",
   "metadata": {},
   "source": [
    "## Other data formats\n",
    "Although netCDF is the de-facto standard in climate science, you may encounter other data formats:\n",
    "##### GRIB\n",
    "##### ZARR\n",
    "##### HDF\n",
    "##### binary\n",
    "Most of these can be opened with xarray, others you may need to combine into a DataSet/DataArray object yourself.\n",
    "\n",
    "### Other netCDF packages and software\n",
    "[Iris](http://scitools.org.uk/iris/) (supported by the UK Met office) provides similar tools for in- memory manipulation of labeled arrays, aimed specifically at weather and climate data needs. Indeed, the Iris Cube was direct inspiration for xarray’s DataArray. xarray and Iris take very different approaches to handling metadata: Iris strictly interprets CF ('Climate and Forecast') conventions. Iris particularly shines at mapping, thanks to its integration with Cartopy.\n",
    "\n",
    "[UV-CDAT](http://uvcdat.llnl.gov/) is another Python library that implements in-memory netCDF-like variables and tools for working with climate data.\n",
    "\n",
    "[Panoply](https://www.giss.nasa.gov/tools/panoply/) is a netCDF, HDF and GRIB data viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70cd60",
   "metadata": {},
   "source": [
    "## Operations\n",
    "We can apply many of the same operations on xarray DataArrays as with pandas Series/DataFrames and numpy ndarrays.\n",
    "\n",
    "### Broadcasting\n",
    "\n",
    "Broadcasting arrays in NumPy can be a nightmare. It is much easier when the data axes are labeled!\n",
    "\n",
    "This is a useless calculation, but it illustrates how perfoming an operation on arrays with differenty coordinates will result in automatic broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_times_lat = ds.lon * ds.lat\n",
    "lon_times_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246628d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_times_lat.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc9de3",
   "metadata": {},
   "source": [
    "### Reductions\n",
    "\n",
    "Just like in NumPy, we can reduce xarray DataArrays along any number of axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa208ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tempanomaly.mean(axis=0).dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d182fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tempanomaly.mean(axis=1).dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c82bd",
   "metadata": {},
   "source": [
    "You see that it is much easier to see over what you are taking the average than it was in NumPy! You can also the dimensions to state over what you want to average:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zonal_mean = ds.mean(dim='lon')\n",
    "ds_zonal_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zonal_mean.tempanomaly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd76bd5",
   "metadata": {},
   "source": [
    "You can also perform a reduction and plot in 1 line of code, such as we've done here of the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zonal_mean.tempanomaly.std('time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070bf99",
   "metadata": {},
   "source": [
    "### Rolling\n",
    "Similar to the `resample` method we used in the previous exercise, we can calculate the running mean to smooth signals. Here, we use a moving average filter of 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds.sel({'lat':0, 'lon':0}, method='nearest').tempanomaly\n",
    "ds_rolling = example.rolling(time=5*12, center=True).mean()\n",
    "example.plot(alpha=.3)\n",
    "ds_rolling.plot()  # notice the missing values at either end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8824fb3",
   "metadata": {},
   "source": [
    "### Access the time\n",
    "You can access time properties with the `dt` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a261",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercises</span>\n",
    "\n",
    "1. Create a dataset with two (10x10x10) DataArrays `A` and `B` with dimensions `lat`, `lon`, `time` and some sensible coordinates. Use `pd.date_range` function to create time coordinates (use the `pd.date_range?` to find out how to use it). Create a `source` attribute that mentions who created the Dataset. Save the dataset as `test2.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a9e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af0a931a",
   "metadata": {},
   "source": [
    "2. Select the time series of temperature anomalies of your birth place (with the latitude and longitude) from the GISSTEMP dataset `ds` that we briefly inspected before.  Use the `resample(time='A').mean()` to create annual means.  Calculate the temperature mean between 1970 and 2000 and use it as a reference. Create the iconic warming stripes graphic. Use [this tutorial](https://matplotlib.org/matplotblog/posts/warming-stripes/) as an inspiration.\n",
    "<img src=\"figures/3b2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013e383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "067a1134",
   "metadata": {},
   "source": [
    "3. Calculate the standard deviation of the monthly and annual temperatures GISSTEMP dataset `ds` and create two maps thereof in a single figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cc732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd8a2bb",
   "metadata": {},
   "source": [
    "4. Calculate the linear trend of a temperature time series above in [$^\\circ$C/century]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4abc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
